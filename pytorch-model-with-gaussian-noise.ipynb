{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d5f1b9360b808d5941d6f37ba2ba20cf3d7f869"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6a1856073e217798a8c5ddd17107a8a72855b665"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from IPython.display import display\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "notebook_start = time.time()\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9898133bb6210b97bead2ed9c46e8280defb305e"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "eaa71cd7a40cf4f647b3af9a5b0fdcd903fefeee",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:  (1306122, 3)\n",
      "Test data dimension:  (56370, 2)\n",
      "\n",
      "Loading data: 0 min 6 sec\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "# Get the target values\n",
    "y_train = train_df[\"target\"].values\n",
    "\n",
    "print(\"Train data dimension: \", train_df.shape)\n",
    "print(\"Test data dimension: \", test_df.shape)\n",
    "\n",
    "since = time.time() - notebook_start\n",
    "print(\"\\nLoading data: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "301805519ef9c9a7d59eb6c6945480d824d6e160"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "559530e8000c8d71eb0c8c6028b1a7209ff88bd9"
   },
   "source": [
    "`seed_torch` sets the seed for numpy and torch to make sure functions with a random component behave deterministically. `torch.backends.cudnn.deterministic = true` sets the CuDNN to deterministic mode.<br>\n",
    "This function allows us to run experiments 100% deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d35808558a9a2d4288ff7993c76f542e87edd62f"
   },
   "outputs": [],
   "source": [
    "SEED = 1209\n",
    "\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e86e673bf0ee05cdea28e4a22faf96dd33bcd30"
   },
   "source": [
    "Sigmoid function in plain numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4ea13f2e32ffa7c050af2c2dd9423ed2aa2592f0"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9d33803fd8fa88f8150eb08b7dea0054bce8d9c"
   },
   "source": [
    "Function to search for best threshold regarding the F1 score given labels and predictions from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4c193ed52242e7e5c921685eef9b683079b467f1"
   },
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001)\n",
    "    F = 2 / (1 / precision + 1 / recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    search_result = {\"threshold\": best_th, \"f1\": best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b8da0d01fd2a9662679af1f70dee9676606dd30"
   },
   "source": [
    "# Analyze Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ab02613b6075c69cba842b988ccafdfad0ab88b6"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b4facb37c5e03202cb75a3b8726fa266d3d93023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings: 4 min 19 sec\n"
     ]
    }
   ],
   "source": [
    "# Load embedding index\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")[:300]\n",
    "\n",
    "\n",
    "# Glove\n",
    "EMBEDDING_FILE = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "glove_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "# Fasttext\n",
    "EMBEDDING_FILE = \"../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\"\n",
    "fast_index = dict(\n",
    "    get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o) > 100\n",
    ")\n",
    "\n",
    "since = time.time() - start\n",
    "print(\"Loading Embeddings: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "684a8dccd44827f42359293456fac83d00f3ecb5"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "265cec208b8700f75d7eed6ccaae7487047fc462"
   },
   "outputs": [],
   "source": [
    "# Utility function to build vocabulary\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d88501650d87df20a4ceabbb95219f3f3ed23a3d"
   },
   "outputs": [],
   "source": [
    "# Utility function to check coverage by embedding\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print(\"Found embeddings for {:.2%} of vocab\".format(len(known_words) / len(vocab)))\n",
    "    print(\n",
    "        \"Found embeddings for  {:.2%} of all text\".format(\n",
    "            nb_known_words / (nb_known_words + nb_unknown_words)\n",
    "        )\n",
    "    )\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6becdb89dfcfb66601393e946c5c80689b6483e5"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_df[\"question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b95737f80b6ac8e58d3058b40b3d73e0eb9d36bc"
   },
   "outputs": [],
   "source": [
    "# function to add lowercase words to embedding index\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:\n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "407f955fe1f1ba43ce2fa57ea2c2dbf28bcc7c50"
   },
   "outputs": [],
   "source": [
    "# function to add lowercase words to embedding index\n",
    "def add_higher(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word.lower() in embedding and word not in embedding:\n",
    "            embedding[word] = embedding[word.lower()]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d44525604257f15e18292ce30602a46f3af12c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15044 words to embedding\n",
      "Added 27176 words to embedding\n"
     ]
    }
   ],
   "source": [
    "add_lower(glove_index, vocab)\n",
    "add_lower(fast_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7fda43fe2ee24456c1fc3f353242d313f0bd44e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2462 words to embedding\n",
      "Added 3703 words to embedding\n"
     ]
    }
   ],
   "source": [
    "add_higher(glove_index, vocab)\n",
    "add_higher(fast_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d72e2514c5c5b8388dacb585876cc478d39c1053"
   },
   "outputs": [],
   "source": [
    "# function to add misspelled words to embedding index\n",
    "def add_misspells(embedding, vocab):\n",
    "    count = 0\n",
    "    misspells = {}\n",
    "    for token in vocab:\n",
    "        new_token = token\n",
    "        if new_token not in embedding:\n",
    "            new_token = re.sub(r\"(.)\\1{2,}\", r\"\\1\", new_token)\n",
    "            if new_token not in embedding:\n",
    "                new_token = re.sub(r\"(.)\\1{1,}\", r\"\\1\", new_token)\n",
    "        if new_token not in embedding:\n",
    "            new_token = nltk.stem.WordNetLemmatizer().lemmatize(new_token)\n",
    "\n",
    "        if new_token in embedding and token not in embedding:\n",
    "            embedding[token] = embedding[new_token]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "7bc4ccae23bb0996ab21a0f239eb6d0b1e202a42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2331 words to embedding\n",
      "Added 3008 words to embedding\n"
     ]
    }
   ],
   "source": [
    "add_misspells(glove_index, vocab)\n",
    "add_misspells(fast_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "51d6d3ed6ec060406aa0e26dbb99bb9db3220ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting vocabulary: 0 min 21 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start\n",
    "print(\"Correcting vocabulary: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93f521adfd488bc3472b3184a3dcff92da0eb3c9"
   },
   "source": [
    "# Processing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9aa7b173563f99dd4352e1874ea41af62bc87c67"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "9a4874c7936cd1e527a82aa779e24fc3de3e23a1"
   },
   "outputs": [],
   "source": [
    "embed_size = 300  # how big is each word vector\n",
    "max_features = 120000  # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70  # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6f5e0b9053f103c08b8715aebbd1ba5a30278674"
   },
   "outputs": [],
   "source": [
    "def spacing_misspell(text):\n",
    "    \"\"\"\n",
    "    'deadbody' -> 'dead body'\n",
    "    \"\"\"\n",
    "    misspell_list = [\"(S|s)hit\", \"(F|f)uck\"]  # ,'Trump'\n",
    "    misspell_re = re.compile(\"(%s)\" % \"|\".join(misspell_list))\n",
    "    return misspell_re.sub(r\" \\1 \", text)\n",
    "\n",
    "\n",
    "def clean_latex(text):\n",
    "    \"\"\"\n",
    "    replace latex math with 'mathematical formula' tag\n",
    "    \"\"\"\n",
    "    corr_t = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = t.strip()\n",
    "        if t != \"\":\n",
    "            corr_t.append(t)\n",
    "    text = \" \".join(corr_t)\n",
    "    text = re.sub(r\"\\[math].+?\\[/math]\", \"mathematical formula\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    unicode string normalization\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "\n",
    "def remove_newline(text):\n",
    "    \"\"\"\n",
    "    remove \\n and  \\t\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(\"\\t\", \" \", text)\n",
    "    text = re.sub(\"\\b\", \" \", text)\n",
    "    text = re.sub(\"\\r\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def decontracted(text):\n",
    "    \"\"\"\n",
    "    de-contract the contraction\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    text = re.sub(r\"(W|w)on(\\'|\\’)t\", \"will not\", text)\n",
    "    text = re.sub(r\"(C|c)an(\\'|\\’)t\", \"can not\", text)\n",
    "    text = re.sub(r\"(Y|y)(\\'|\\’)all\", \"you all\", text)\n",
    "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll\", \"you all\", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"(I|i)(\\'|\\’)m\", \"i am\", text)\n",
    "    text = re.sub(r\"(A|a)in(\\'|\\’)t\", \"is not\", text)\n",
    "    text = re.sub(r\"n(\\'|\\’)t\", \" not\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)re\", \" are\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)s\", \" is\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)d\", \" would\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ll\", \" will\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)t\", \" not\", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ve\", \" have\", text)\n",
    "\n",
    "    # quora\n",
    "    text = re.sub(r\"(Q|q)uoran\", \"quora contributor\", text)\n",
    "    text = re.sub(r\"(Q|q)uorans\", \"quora contributors\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    regular_punct = list(string.punctuation)\n",
    "    extra_punct = [\n",
    "        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "    all_punct = \"\".join(sorted(list(set(regular_punct + extra_punct))))\n",
    "    re_tok = re.compile(f\"([{all_punct}])\")\n",
    "    return re_tok.sub(r\" \\1 \", text)\n",
    "\n",
    "\n",
    "def spacing_digit(text):\n",
    "    \"\"\"\n",
    "    add space before and after digits\n",
    "    \"\"\"\n",
    "    re_tok = re.compile(\"([0-9])\")\n",
    "    return re_tok.sub(r\" \\1 \", text)\n",
    "\n",
    "\n",
    "def spacing_number(text):\n",
    "    \"\"\"\n",
    "    add space before and after numbers\n",
    "    \"\"\"\n",
    "    re_tok = re.compile(\"([0-9]{1,})\")\n",
    "    return re_tok.sub(r\" \\1 \", text)\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    \"\"\"\n",
    "    numbers are not toxic\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\d+\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\s+$\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def substitute(text):\n",
    "    \"\"\"\n",
    "    substitute some words after de-contraction\n",
    "    \"\"\"\n",
    "    # text = re.sub(r\" e g \", \" eg \", text)\n",
    "    # text = re.sub(r\" b g \", \" bg \", text)\n",
    "    # text = re.sub(r\" u s \", \" US \", text)\n",
    "    # text = re.sub(r\" u s a \", \" USA \", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "35830415a68786b1b4e5d71270d9e72d35a65cd2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "additional features from \n",
    "https://github.com/thinline72/toxic/blob/master/skolbachev/toxic/tokenizers/glove_twitter_tokenizer.py\n",
    "\"\"\"\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    result = \"<hashtag> \" + hashtag_body.lower()\n",
    "    return result\n",
    "\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def add_feats(\n",
    "    text,\n",
    "    numbers=False,\n",
    "    smiley=False,\n",
    "    twitter=False,\n",
    "    allcaps=True,\n",
    "    replong=False,\n",
    "    FLAGS=FLAGS,\n",
    "):\n",
    "\n",
    "    # Different regex parts for smiley faces\n",
    "    if smiley:\n",
    "        eyes = r\"[8:=;]\"\n",
    "        nose = r\"['`\\-]?\"\n",
    "        text = re.sub(\n",
    "            r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes),\n",
    "            \"<smile>\",\n",
    "            text,\n",
    "            flags=FLAGS,\n",
    "        )\n",
    "        text = re.sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\", text, flags=FLAGS)\n",
    "        text = re.sub(\n",
    "            r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes),\n",
    "            \"<sadface>\",\n",
    "            text,\n",
    "            flags=FLAGS,\n",
    "        )\n",
    "        text = re.sub(\n",
    "            r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\", text, flags=FLAGS\n",
    "        )\n",
    "        text = re.sub(r\"<3\", \"<heart>\", text, flags=FLAGS)\n",
    "\n",
    "    if numbers:\n",
    "        text = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\", text, flags=FLAGS)\n",
    "\n",
    "    if twitter:\n",
    "        text = re.sub(r\"#(\\w+)\", hashtag, text, flags=FLAGS)\n",
    "        text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\", text, flags=FLAGS)\n",
    "        text = re.sub(r\"@\\w+\", \"<user>\", text, flags=FLAGS)\n",
    "\n",
    "    if allcaps:\n",
    "        try:\n",
    "            text = re.sub(r\"([A-Z]){2,}\", allcaps, text, flags=FLAGS)\n",
    "        except TypeError:\n",
    "            print(text)\n",
    "\n",
    "    if replong:\n",
    "        text = re.sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\", text, flags=FLAGS)\n",
    "        text = re.sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\", text, flags=FLAGS)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "2e275dd90089f414355b1019789ad8484d0b4172"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, add_features=False, sub=False, remove_num=True):\n",
    "    \"\"\"\n",
    "    preprocess text into clean text for tokenization\n",
    "    NOTE:\n",
    "        1. glove supports uppper case words\n",
    "        2. glove supports digit\n",
    "        3. glove supports punctuation\n",
    "        5. glove supports domains e.g. www.apple.com\n",
    "        6. glove supports misspelled words e.g. FUCKKK\n",
    "    \"\"\"\n",
    "    ## add tags\n",
    "    if add_features:\n",
    "        text = add_feats(text)\n",
    "    ## remove new line\n",
    "    text = remove_newline(text)\n",
    "    ## de-contract\n",
    "    text = decontracted(text)\n",
    "    ## space misspell\n",
    "    text = spacing_misspell(text)\n",
    "    ## clean_latex\n",
    "    text = clean_latex(text)\n",
    "    ## space\n",
    "    text = spacing_punctuation(text)\n",
    "    ## substitute after decontract\n",
    "    if sub:\n",
    "        text = substitute(text)\n",
    "    ## handle numbers\n",
    "    if remove_num:\n",
    "        text = remove_number(text)\n",
    "    else:\n",
    "        text = spacing_number(text)\n",
    "        text = spacing_digit(text)\n",
    "    # 9. remove space\n",
    "    text = remove_space(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ae77fc20696dc83140f74f00243e002b0cf54b8"
   },
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "4c04eaeb0438991a69c81770ffa1d59c933b548a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_features(df):\n",
    "\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: str(x))\n",
    "    df[\"total_length\"] = df[\"question_text\"].apply(len)\n",
    "    df[\"capitals\"] = df[\"question_text\"].apply(\n",
    "        lambda comment: sum(1 for c in comment if c.isupper())\n",
    "    )\n",
    "    df[\"caps_vs_length\"] = df.apply(\n",
    "        lambda row: float(row[\"capitals\"]) / float(row[\"total_length\"]), axis=1\n",
    "    )\n",
    "    df[\"num_words\"] = df.question_text.str.count(\"\\S+\")\n",
    "    df[\"num_unique_words\"] = df[\"question_text\"].apply(\n",
    "        lambda comment: len(set(w for w in comment.split()))\n",
    "    )\n",
    "    df[\"words_vs_unique\"] = df[\"num_unique_words\"] / df[\"num_words\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_features(train_df)\n",
    "test = add_features(test_df)\n",
    "\n",
    "\n",
    "features = train[[\"caps_vs_length\", \"words_vs_unique\"]].fillna(0)\n",
    "test_features = test[[\"caps_vs_length\", \"words_vs_unique\"]].fillna(0)\n",
    "\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "163b6476ab43d552d05a646bc42beda53e8c0947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding meta features: 0 min 48 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start\n",
    "print(\"Adding meta features: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01b8cfb58a48b69db5869fe15c6b9a5e5e123d09"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "6fa708b5bd76943872fc2e92d1d66898394d1899"
   },
   "outputs": [],
   "source": [
    "start_preprocessing = time.time()\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "c0f3a9ecd216eddc7621915276b65a9fe5e63ad2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e602e5b70103458db99b615c999db81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab6fe9d212c402cb14df4504dc5124b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56370), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = train_df[\"question_text\"].progress_apply(preprocess)\n",
    "x_test = test_df[\"question_text\"].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "d50a5387bcf6b8e34f307b9a2eeaa1ce99046f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add lower after preprocessing:\n",
      "Added 5819 words to embedding\n",
      "Added 9474 words to embedding\n",
      "\n",
      "Add misspells after preprocessing:\n",
      "Added 809 words to embedding\n",
      "Added 953 words to embedding\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(x_train)\n",
    "\n",
    "print(\"Add lower after preprocessing:\")\n",
    "add_lower(glove_index, vocab)\n",
    "add_lower(fast_index, vocab)\n",
    "\n",
    "print(\"\\nAdd misspells after preprocessing:\")\n",
    "add_misspells(glove_index, vocab)\n",
    "add_misspells(fast_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "18b5e65178732ea8ea11b13c449d3db7446ee4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 794 words to embedding\n",
      "Added 1289 words to embedding\n"
     ]
    }
   ],
   "source": [
    "add_higher(glove_index, vocab)\n",
    "add_higher(fast_index, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "9fb4bb39ff9b3f14a9891e60c9e9d2b8566c85fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vocab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ae18f267946a3c6dc64aff1b6083de2b24077a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing: 2 min 53 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start\n",
    "print(\"Text preprocessing: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de4a4d394f2601659525b4a418fa9625237107eb"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "1d49ae6a8f99d4c74405732f6b41b443e95aa46a"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "f81dbfbfb33b991fa83cbcd22b10405fa5b15195"
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features, filters=\"\", lower=False, split=\" \")\n",
    "# fit to data\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "# tokenize the texts into sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "# Pad the sentences\n",
    "x_train = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "x_test = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "# shuffling the data\n",
    "np.random.seed(SEED)\n",
    "trn_idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[trn_idx]\n",
    "y_train = y_train[trn_idx]\n",
    "features = features[trn_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "42335659fb9d1e9533c42e083c917319562e39af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: 0 min 50 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start\n",
    "print(\"Tokenization: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "76dff4395979adfc046521fb8b3157afadc8e566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total preprocessing time: 3 min 43 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start_preprocessing\n",
    "print(\"Total preprocessing time: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8607fe327a4d7f2af0dfad02cb7fae0a14b0f2b4"
   },
   "source": [
    "# Creating the embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a08d07cb08243af3fe09204d923f3c766e73e5ab"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "e5df8649c4db0a6e5bbf05088614c8cde4985081"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index, embedding_index):\n",
    "\n",
    "    all_embs = np.stack(embedding_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "63c2416d2145af3eacc98a01c8b9e536b99a6de6"
   },
   "outputs": [],
   "source": [
    "def load_fast(word_index, embeddings_index):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "9f47acd3b1d4bec89a16fda89f3d042e44ea7de3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 600)\n",
      "Embedding matrices time: 0 min 18 sec\n"
     ]
    }
   ],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "seed_everything()\n",
    "\n",
    "glove_matrix = load_glove(tokenizer.word_index, glove_index)\n",
    "fast_matrix = load_fast(tokenizer.word_index, fast_index)\n",
    "\n",
    "embedding_matrix = np.concatenate(([glove_matrix, fast_matrix]), axis=1)\n",
    "\n",
    "del glove_index\n",
    "del glove_matrix\n",
    "del fast_index\n",
    "del fast_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(np.shape(embedding_matrix))\n",
    "\n",
    "since = time.time() - start\n",
    "print(\"Embedding matrices time: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd3804f9bf44762f7715e23099d42f606a4c281b"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "65e60c725db67aed7872f852603211cad7eb6ae4"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "a3e084b4fe81baf1bc392d36e2d43b2f6e032331"
   },
   "outputs": [],
   "source": [
    "splits = list(\n",
    "    StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(x_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "d1ecc5db0bbe56f59e55d1509358959306c2d0b9"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "c78fe9f2abf0db1b3aaf39b97d9303bf03c24d18"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(x.contiguous().view(-1, feature_dim), self.weight).view(\n",
    "            -1, step_dim\n",
    "        )\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "f0c96d213c4e05972333fe062f9586b0dd2ad5c3"
   },
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        base_lr=1e-3,\n",
    "        max_lr=6e-3,\n",
    "        step_size=2000,\n",
    "        factor=0.6,\n",
    "        min_lr=1e-4,\n",
    "        mode=\"triangular\",\n",
    "        gamma=1.0,\n",
    "        scale_fn=None,\n",
    "        scale_mode=\"cycle\",\n",
    "        last_batch_iteration=-1,\n",
    "    ):\n",
    "\n",
    "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
    "            raise TypeError(\"{} is not an Optimizer\".format(type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\n",
    "                    \"expected {} base_lr, got {}\".format(\n",
    "                        len(optimizer.param_groups), len(base_lr)\n",
    "                    )\n",
    "                )\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\n",
    "                    \"expected {} max_lr, got {}\".format(\n",
    "                        len(optimizer.param_groups), len(max_lr)\n",
    "                    )\n",
    "                )\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in [\"triangular\", \"triangular2\", \"exp_range\"] and scale_fn is None:\n",
    "            raise ValueError(\"mode is invalid and scale_fn is None\")\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == \"triangular\":\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = \"cycle\"\n",
    "            elif self.mode == \"triangular2\":\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = \"cycle\"\n",
    "            elif self.mode == \"exp_range\":\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = \"iterations\"\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "        self.last_loss = np.inf\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss > self.last_loss:\n",
    "            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n",
    "            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.0\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2.0 ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma ** (x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == \"cycle\":\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "4266825fbf77ffc6697ac46c3f99bfbbe4ab5caf"
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Gaussian noise regularizer.\n",
    "\n",
    "    Args:\n",
    "        sigma (float, optional): relative standard deviation used to generate the\n",
    "            noise. Relative means that it will be multiplied by the magnitude of\n",
    "            the value your are adding the noise to. This means that sigma can be\n",
    "            the same regardless of the scale of the vector.\n",
    "        is_relative_detach (bool, optional): whether to detach the variable before\n",
    "            computing the scale of the noise. If `False` then the scale of the noise\n",
    "            won't be seen as a constant but something to optimize: this will bias the\n",
    "            network to generate vectors with smaller values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma=0.07, is_relative_detach=True):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.is_relative_detach = is_relative_detach\n",
    "        self.noise = torch.tensor(0).to(torch.device(\"cuda\")).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.sigma != 0:\n",
    "            scale = (\n",
    "                self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
    "            )\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
    "            x = x + sampled_noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d03b225c099f9ebc2f71ec9b71caac69ffd3a9fe"
   },
   "source": [
    "Now define the neural network. Defining a neural network in PyTorch is done by defining a class. This is almost as intuitive as Keras. The main difference is that you have one function (`__init__`) where it is defined which layers there are in the network and another function (`forward`) which defines the flow of data through the net.\n",
    "\n",
    "I replicated the architecture used in [@Shujian Liu's kernel](https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr) in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9836dce4166c789db5f2d536609dee4c658d6a3"
   },
   "source": [
    "# Neural Net Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "bd845c91ea3fdf037b26995a30065353f0f5470c"
   },
   "outputs": [],
   "source": [
    "h_size = 64\n",
    "dense_out = False\n",
    "embedding_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "313b59ffab2f076409a4bd4e680e3b49e9c923a3"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        hidden_size = h_size\n",
    "        emb_dropout = embedding_dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(max_features, embed_size * 2)\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.gaussian_noise = GaussianNoise()\n",
    "        self.embedding_dropout = nn.Dropout(emb_dropout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size * 2, hidden_size, bidirectional=True, batch_first=True\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size * 2, hidden_size, bidirectional=True, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 8 + 2, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        self.out2 = nn.Linear(hidden_size * 8 + 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        dense = dense_out\n",
    "\n",
    "        h_embedding = self.embedding(x[0])\n",
    "        h_embedding = self.gaussian_noise(h_embedding)\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0))\n",
    "        )\n",
    "\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "\n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "\n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()\n",
    "\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool, f), 1)\n",
    "\n",
    "        if dense:\n",
    "            conc = self.relu(self.linear(conc))\n",
    "            conc = self.dropout(conc)\n",
    "            out = self.out(conc)\n",
    "        else:\n",
    "            out = self.out2(conc)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5445e83f96ca47abded343f42ed702ad5ecaca7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "51453ae35fce480b307c64c4e8a82828e785001f"
   },
   "outputs": [],
   "source": [
    "batch_size = 512  # how many samples to process at once\n",
    "n_epochs = 5  # how many times to iterate over all samples\n",
    "\n",
    "# scheduler parameters\n",
    "step_size = 1000\n",
    "base_lr, max_lr = 0.001, 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0939642af666800847c284bf924759c693ea1e83"
   },
   "source": [
    "Now we can already train the network. Unfortunately, we do not have an API as high-level as keras's `.fit` in PyTorch. However, the code is still not too complicated and I have added comments where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "b7ad1398071649e90a0f08c85535c03f8e23aa0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=0.1154 \t val_loss=0.1024 \t time=229.36s\n",
      "Epoch 2/5 \t loss=0.1000 \t val_loss=0.0959 \t time=232.44s\n",
      "Epoch 3/5 \t loss=0.0950 \t val_loss=0.0949 \t time=233.03s\n",
      "Epoch 4/5 \t loss=0.0911 \t val_loss=0.0947 \t time=233.05s\n",
      "Epoch 5/5 \t loss=0.0879 \t val_loss=0.0949 \t time=232.56s\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=0.1155 \t val_loss=0.1003 \t time=234.02s\n",
      "Epoch 2/5 \t loss=0.1002 \t val_loss=0.0967 \t time=233.27s\n",
      "Epoch 3/5 \t loss=0.0950 \t val_loss=0.0957 \t time=232.96s\n",
      "Epoch 4/5 \t loss=0.0914 \t val_loss=0.0955 \t time=232.74s\n",
      "Epoch 5/5 \t loss=0.0881 \t val_loss=0.0952 \t time=232.82s\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=0.1158 \t val_loss=0.1032 \t time=232.77s\n",
      "Epoch 2/5 \t loss=0.1000 \t val_loss=0.0978 \t time=233.86s\n",
      "Epoch 3/5 \t loss=0.0947 \t val_loss=0.0961 \t time=234.83s\n",
      "Epoch 4/5 \t loss=0.0908 \t val_loss=0.0958 \t time=232.84s\n",
      "Epoch 5/5 \t loss=0.0874 \t val_loss=0.0963 \t time=232.31s\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=0.1163 \t val_loss=0.0996 \t time=231.18s\n",
      "Epoch 2/5 \t loss=0.1004 \t val_loss=0.0968 \t time=229.75s\n",
      "Epoch 3/5 \t loss=0.0951 \t val_loss=0.0943 \t time=229.61s\n",
      "Epoch 4/5 \t loss=0.0912 \t val_loss=0.0939 \t time=229.59s\n",
      "Epoch 5/5 \t loss=0.0880 \t val_loss=0.0943 \t time=229.54s\n",
      "Fold 5\n",
      "Epoch 1/5 \t loss=0.1160 \t val_loss=0.1017 \t time=229.67s\n",
      "Epoch 2/5 \t loss=0.0995 \t val_loss=0.0976 \t time=229.48s\n",
      "Epoch 3/5 \t loss=0.0946 \t val_loss=0.0967 \t time=229.58s\n",
      "Epoch 4/5 \t loss=0.0906 \t val_loss=0.0954 \t time=229.57s\n",
      "Epoch 5/5 \t loss=0.0876 \t val_loss=0.0952 \t time=229.63s\n",
      "\n",
      "Total loss = 0.0878 \t val_loss=0.0952\n"
     ]
    }
   ],
   "source": [
    "# matrix for the out-of-fold predictions\n",
    "train_preds = np.zeros((len(x_train)))\n",
    "# matrix for the predictions on the test set\n",
    "test_preds = np.zeros((len(x_test)))\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "features = np.array(features)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(\n",
    "        y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32\n",
    "    ).cuda()\n",
    "\n",
    "    kfold_X_features = features[train_idx.astype(int)]\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(\n",
    "        y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32\n",
    "    ).cuda()\n",
    "\n",
    "    model = NeuralNet()\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # define binary cross entropy loss\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick\n",
    "    # for numerical stability in the loss\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=base_lr\n",
    "    )\n",
    "\n",
    "    scheduler = CyclicLR(\n",
    "        optimizer,\n",
    "        base_lr=base_lr,\n",
    "        max_lr=max_lr,\n",
    "        step_size=step_size,\n",
    "        mode=\"exp_range\",\n",
    "        gamma=0.99994,\n",
    "    )\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "\n",
    "    train = MyDataset(train)\n",
    "    valid = MyDataset(valid)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    print(f\"Fold {i + 1}\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.0\n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            f = kfold_X_features[index]\n",
    "            y_pred = model([x_batch, f])\n",
    "\n",
    "            scheduler.batch_step()\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "\n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(x_test)))\n",
    "\n",
    "        avg_val_loss = 0.0\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]\n",
    "            y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size : (i + 1) * batch_size] = sigmoid(\n",
    "                y_pred.cpu().numpy()\n",
    "            )[:, 0]\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\n",
    "            \"Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s\".format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss)\n",
    "\n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size : (i + 1) * batch_size]\n",
    "        y_pred = model([x_batch, f]).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size : (i + 1) * batch_size] = sigmoid(\n",
    "            y_pred.cpu().numpy()\n",
    "        )[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "print(\n",
    "    \"\\nTotal loss = {:.4f} \\t val_loss={:.4f}\".format(\n",
    "        np.average(avg_losses_f), np.average(avg_val_losses_f)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "6fa93d18ef925d28bf30d6f0826b97d136310780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del x_train, x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "455cfc29f5656068e27f8e8e5375977caff902bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Neural Network: 97 min 1 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - start\n",
    "print(\n",
    "    \"Training the Neural Network: {:.0f} min {:.0f} sec\".format(since // 60, since % 60)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcf0e8e956e78af53b554c7840da4bf469460320"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "07056f2dabeabde7adb5a7f6686087353e5d3ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'threshold': 0.36109811067581177, 'f1': 0.6938563299731376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "search_result = threshold_search(y_train, train_preds)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8955d3ef4006e7cb358d637c4190715b418e18ef"
   },
   "source": [
    "Finally submit the predictions with the threshold we have just found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "e87e9557c51794f9744e01abc27c6ff380422845"
   },
   "outputs": [],
   "source": [
    "submission = test_df[[\"qid\"]].copy()\n",
    "submission[\"prediction\"] = test_preds > search_result[\"threshold\"]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "80a90cd8110f116904bc3e1eafa35945433d5dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notebook runtime: 106 min 37 sec\n"
     ]
    }
   ],
   "source": [
    "since = time.time() - notebook_start\n",
    "print(\"Total notebook runtime: {:.0f} min {:.0f} sec\".format(since // 60, since % 60))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
